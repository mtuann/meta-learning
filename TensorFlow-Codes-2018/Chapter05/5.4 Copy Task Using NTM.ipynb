{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy Task Using NTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see, how to perform copy tasks using NTM. The goal of the copy task is to see how NTM stores and recall the sequence of arbitrary length. We will feed the network a random sequence along with a marker indicating the end of a sequence. It has to learn to output the given input sequence. So, the network will store the input sequence to the memory and then it will read back from the memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define a class NTMCell where we will implement our neural turing machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NTMCell():\n",
    "    def __init__(self, rnn_size, memory_size, memory_vector_dim, read_head_num, write_head_num,\n",
    "                 addressing_mode='content_and_location', shift_range=1, reuse=False, output_dim=None):\n",
    "        \n",
    "        #initialize all the variables\n",
    "        self.rnn_size = rnn_size\n",
    "        self.memory_size = memory_size\n",
    "        self.memory_vector_dim = memory_vector_dim\n",
    "        self.read_head_num = read_head_num\n",
    "        self.write_head_num = write_head_num\n",
    "        self.addressing_mode = addressing_mode\n",
    "        self.reuse = reuse\n",
    "        self.step = 0\n",
    "        self.output_dim = output_dim\n",
    "        self.shift_range = shift_range\n",
    "        \n",
    "        #initialize controller as the basic rnn cell\n",
    "        self.controller = tf.nn.rnn_cell.BasicRNNCell(self.rnn_size)\n",
    "\n",
    "        \n",
    "        \n",
    "    def __call__(self, x, prev_state):\n",
    "        \n",
    "        \n",
    "        prev_read_vector_list = prev_state['read_vector_list']  \n",
    "        prev_controller_state = prev_state['controller_state']     \n",
    "    \n",
    "        controller_input = tf.concat([x] + prev_read_vector_list, axis=1)\n",
    "        \n",
    "        #next we pass the controller which is the RNN cell, the controller_input and prev_controller_state\n",
    "        with tf.variable_scope('controller', reuse=self.reuse):\n",
    "            controller_output, controller_state = self.controller(controller_input, prev_controller_state)\n",
    "\n",
    "        #initialize read and write heads    \n",
    "        num_parameters_per_head = self.memory_vector_dim + 1 + 1 + (self.shift_range * 2 + 1) + 1\n",
    "        num_heads = self.read_head_num + self.write_head_num\n",
    "        \n",
    "        total_parameter_num = num_parameters_per_head * num_heads + self.memory_vector_dim * 2 * self.write_head_num\n",
    "        \n",
    "        #initialize weight matrix and bias and compute the parameters\n",
    "        with tf.variable_scope(\"o2p\", reuse=(self.step > 0) or self.reuse):\n",
    "            o2p_w = tf.get_variable('o2p_w', [controller_output.get_shape()[1], total_parameter_num],\n",
    "                                    initializer=tf.random_normal_initializer(mean=0.0, stddev=0.5))\n",
    "            o2p_b = tf.get_variable('o2p_b', [total_parameter_num],\n",
    "                                    initializer=tf.random_normal_initializer(mean=0.0, stddev=0.5))\n",
    "            parameters = tf.nn.xw_plus_b(controller_output, o2p_w, o2p_b)\n",
    "            \n",
    "            \n",
    "        head_parameter_list = tf.split(parameters[:, :num_parameters_per_head * num_heads], num_heads, axis=1)\n",
    "        \n",
    "        erase_add_list = tf.split(parameters[:, num_parameters_per_head * num_heads:], 2 * self.write_head_num, axis=1)\n",
    "\n",
    "        \n",
    "        #previous write weight vector\n",
    "        prev_w_list = prev_state['w_list'] \n",
    "        \n",
    "        #previous Memory\n",
    "        prev_M = prev_state['M']\n",
    "        \n",
    "        w_list = []\n",
    "        p_list = []\n",
    "        \n",
    "        \n",
    "        #now, we will initialize some of the important parameters that we use for addressing. \n",
    "        for i, head_parameter in enumerate(head_parameter_list):\n",
    "\n",
    "            #key vector\n",
    "            k = tf.tanh(head_parameter[:, 0:self.memory_vector_dim])\n",
    "            \n",
    "            #key strength(beta)\n",
    "            beta = tf.sigmoid(head_parameter[:, self.memory_vector_dim]) * 10  \n",
    "            \n",
    "            #interpolation gate\n",
    "            g = tf.sigmoid(head_parameter[:, self.memory_vector_dim + 1])\n",
    "            \n",
    "            #shift matrix  \n",
    "            s = tf.nn.softmax(\n",
    "                head_parameter[:, self.memory_vector_dim + 2:self.memory_vector_dim + 2 + (self.shift_range * 2 + 1)]\n",
    "            )\n",
    "            \n",
    "            #sharpening factor\n",
    "            gamma = tf.log(tf.exp(head_parameter[:, -1]) + 1) + 1\n",
    "            \n",
    "            with tf.variable_scope('addressing_head_%d' % i):\n",
    "                w = self.addressing(k, beta, g, s, gamma, prev_M, prev_w_list[i]) \n",
    "\n",
    "            w_list.append(w)\n",
    "            p_list.append({'k': k, 'beta': beta, 'g': g, 's': s, 'gamma': gamma})\n",
    "\n",
    "        \n",
    "        \n",
    "        #We basically perform two important operations in NTM one is read and other is write operation.\n",
    "    \n",
    "        \n",
    "        #A. Read Operation:\n",
    "            # read operation is the linear combination  of weights and memory. \n",
    "        \n",
    "        #select the head to read from\n",
    "        read_w_list = w_list[:self.read_head_num]\n",
    "        \n",
    "        \n",
    "        read_vector_list = []\n",
    "        for i in range(self.read_head_num):  \n",
    "            #linear combination  of the weights and memory\n",
    "            read_vector = tf.reduce_sum(tf.expand_dims(read_w_list[i], dim=2) * prev_M, axis=1)\n",
    "            read_vector_list.append(read_vector)\n",
    "            \n",
    "\n",
    "        #B. Write Operation\n",
    "        \n",
    "            #Unlike read operation, write operation consists of two steps - erase and add. \n",
    "        \n",
    "        #select the head to write\n",
    "        write_w_list = w_list[self.read_head_num:]\n",
    "        M = prev_M\n",
    "        for i in range(self.write_head_num):\n",
    "            \n",
    "            #the erase vector will be multipled with weight vector to denote which location to erase \n",
    "            w = tf.expand_dims(write_w_list[i], axis=2)\n",
    "            erase_vector = tf.expand_dims(tf.sigmoid(erase_add_list[i * 2]), axis=1)\n",
    "            \n",
    "            #next we perform the add operation\n",
    "            add_vector = tf.expand_dims(tf.tanh(erase_add_list[i * 2 + 1]), axis=1)\n",
    "            M = M * (tf.ones(M.get_shape()) - tf.matmul(w, erase_vector)) + tf.matmul(w, add_vector)\n",
    "            \n",
    "    \n",
    "        \n",
    "        #controller output\n",
    "        if not self.output_dim:\n",
    "            output_dim = x.get_shape()[1]\n",
    "        else:\n",
    "            output_dim = self.output_dim\n",
    "            \n",
    "        with tf.variable_scope(\"o2o\", reuse=(self.step > 0) or self.reuse):\n",
    "            o2o_w = tf.get_variable('o2o_w', [controller_output.get_shape()[1], output_dim],\n",
    "                                    initializer=tf.random_normal_initializer(mean=0.0, stddev=0.5))\n",
    "            o2o_b = tf.get_variable('o2o_b', [output_dim],\n",
    "                                    initializer=tf.random_normal_initializer(mean=0.0, stddev=0.5))\n",
    "            NTM_output = tf.nn.xw_plus_b(controller_output, o2o_w, o2o_b)\n",
    "\n",
    "        state = {\n",
    "            'controller_state': controller_state,\n",
    "            'read_vector_list': read_vector_list,\n",
    "            'w_list': w_list,\n",
    "            'p_list': p_list,\n",
    "            'M': M\n",
    "        }\n",
    "\n",
    "        self.step += 1\n",
    "        \n",
    "        return NTM_output, state\n",
    "\n",
    "    \n",
    "    \n",
    "    def addressing(self, k, beta, g, s, gamma, prev_M, prev_w):\n",
    "        \"\"\"\n",
    "        We will implement two different addressing mechanisms,\n",
    "        \n",
    "        1. Content-based\n",
    "        2. Location-based\n",
    "            2.1. Interpolation\n",
    "            2.2. Convolutional Shift\n",
    "            2.3. Sharpening\n",
    "        \"\"\"\n",
    "       \n",
    "        \n",
    "        #1. Content based addressing\n",
    "        \n",
    "        #contoller returns a key vector K that is compared with the each row in the memory M using cosine similarity\n",
    "        k = tf.expand_dims(k, axis=2)\n",
    "        inner_product = tf.matmul(prev_M, k)\n",
    "        \n",
    "        k_norm = tf.sqrt(tf.reduce_sum(tf.square(k), axis=1, keepdims=True))\n",
    "        M_norm = tf.sqrt(tf.reduce_sum(tf.square(prev_M), axis=2, keepdims=True))\n",
    "        norm_product = M_norm * k_norm\n",
    "        \n",
    "        #Compute cosine similarity\n",
    "        K = tf.squeeze(inner_product / (norm_product + 1e-8))                     \n",
    "    \n",
    "        #now, we produce the normalized weight vector based on the similairty and the key strength (beta)\n",
    "        #beta is used for adjusting the precision on the head focus\n",
    "        \n",
    "        K_amplified = tf.exp(tf.expand_dims(beta, axis=1) * K)\n",
    "        w_c = K_amplified / tf.reduce_sum(K_amplified, axis=1, keepdims=True)       \n",
    "    \n",
    "\n",
    "        if self.addressing_mode == 'content':                                   \n",
    "            return w_c\n",
    "\n",
    "        \n",
    "        #2. Location based addressing\n",
    "        \n",
    "        #location based addressing involves three other steps,\n",
    "        #2.1 Interpolation\n",
    "        #2.2 Convolutional Shift\n",
    "        #2.3 Sharpening\n",
    "\n",
    "        \n",
    "        #2.1 Interpolation \n",
    "        \n",
    "        #The first step in the location based addressing is the interpolation.\n",
    "        #It is used to decide whether we should use the weights we obtained at the previous time step  or\n",
    "        #we should use the weights obtained through content based addressing. \n",
    "        #But how do we decide that? We use a new scalar parameter which is used for determining\n",
    "        #which weights we should use. \n",
    "\n",
    "        \n",
    "        g = tf.expand_dims(g, axis=1)\n",
    "        w_g = g * w_c + (1 - g) * prev_w                                       \n",
    "        \n",
    "        \n",
    "        #2.2 Convolutional Shift\n",
    "        #The next step is called convolution shit. It is used for moving the head position.\n",
    "        #i.e it is used for shifting the focus from one location to the another. \n",
    "        \n",
    "        s = tf.concat([s[:, :self.shift_range + 1],\n",
    "                       tf.zeros([s.get_shape()[0], self.memory_size - (self.shift_range * 2 + 1)]),\n",
    "                       s[:, -self.shift_range:]], axis=1)\n",
    "        \n",
    "        t = tf.concat([tf.reverse(s, axis=[1]), tf.reverse(s, axis=[1])], axis=1)\n",
    "        \n",
    "        s_matrix = tf.stack(\n",
    "            [t[:, self.memory_size - i - 1:self.memory_size * 2 - i - 1] for i in range(self.memory_size)],\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        w_ = tf.reduce_sum(tf.expand_dims(w_g, axis=1) * s_matrix, axis=2)      # eq (8)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        #2.3 Sharpening\n",
    "                \n",
    "        #The final step is sharpening. As a result of the convolutional shift, the weights will not be sharp.\n",
    "        #i.e because of the shift, weights focused at a single location will be dispersed into other locations.\n",
    "        #To mitigate this effect, we perform sharpening. \n",
    "        #We use another parameter called  gamma to perform sharpening and it can be expressed as,\n",
    "\n",
    "        \n",
    "        w_sharpen = tf.pow(w_, tf.expand_dims(gamma, axis=1))\n",
    "        w = w_sharpen / tf.reduce_sum(w_sharpen, axis=1, keepdims=True)\n",
    "\n",
    "        return w\n",
    "\n",
    "    \n",
    "    \n",
    "    #next we define the function called zero state for initializing all the states - \n",
    "    #controller state, read vector, weights and memory\n",
    "    \n",
    "    def zero_state(self, batch_size, dtype):\n",
    "        def expand(x, dim, N):\n",
    "            return tf.concat([tf.expand_dims(x, dim) for _ in range(N)], axis=dim)\n",
    "\n",
    "        with tf.variable_scope('init', reuse=self.reuse):\n",
    "            state = {\n",
    "                'controller_state': expand(tf.tanh(tf.get_variable('init_state', self.rnn_size,\n",
    "                                            initializer=tf.random_normal_initializer(mean=0.0, stddev=0.5))),\n",
    "                                  dim=0, N=batch_size),\n",
    "                \n",
    "                'read_vector_list': [expand(tf.nn.softmax(tf.get_variable('init_r_%d' % i, [self.memory_vector_dim],\n",
    "                                            initializer=tf.random_normal_initializer(mean=0.0, stddev=0.5))),\n",
    "                                  dim=0, N=batch_size)\n",
    "                           for i in range(self.read_head_num)],\n",
    "                \n",
    "                'w_list': [expand(tf.nn.softmax(tf.get_variable('init_w_%d' % i, [self.memory_size],\n",
    "                                            initializer=tf.random_normal_initializer(mean=0.0, stddev=0.5))),\n",
    "                                  dim=0, N=batch_size) if self.addressing_mode == 'content_and_loaction'\n",
    "                           else tf.zeros([batch_size, self.memory_size])\n",
    "                           for i in range(self.read_head_num + self.write_head_num)],\n",
    "                \n",
    "                'M': expand(tf.tanh(tf.get_variable('init_M', [self.memory_size, self.memory_vector_dim],\n",
    "                                            initializer=tf.random_normal_initializer(mean=0.0, stddev=0.5))),\n",
    "                                  dim=0, N=batch_size)\n",
    "            }\n",
    "            return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function called generate random sequence which will generate random sequence of length seq_length and we will feed this seqence to the NTM input for the copy task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_random_strings(batch_size, seq_length, vector_dim):\n",
    "    return np.random.randint(0, 2, size=[batch_size, seq_length, vector_dim]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create NTMCopyModel for performing the whole copy task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NTMCopyModel():\n",
    "    \n",
    "    def __init__(self, args, seq_length, reuse=False):\n",
    "        \n",
    "        #input sequence\n",
    "        self.x = tf.placeholder(name='x', dtype=tf.float32, shape=[args.batch_size, seq_length, args.vector_dim])\n",
    "        \n",
    "        #output sequence\n",
    "        self.y = self.x\n",
    "        \n",
    "        #end of the sequence\n",
    "        eof = np.zeros([args.batch_size, args.vector_dim + 1])\n",
    "        eof[:, args.vector_dim] = np.ones([args.batch_size])\n",
    "        eof = tf.constant(eof, dtype=tf.float32)\n",
    "        zero = tf.constant(np.zeros([args.batch_size, args.vector_dim + 1]), dtype=tf.float32)\n",
    "        \n",
    "        if args.model == 'LSTM':\n",
    "            def rnn_cell(rnn_size):\n",
    "                return tf.nn.rnn_cell.BasicLSTMCell(rnn_size, reuse=reuse)\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(args.rnn_size) for _ in range(args.rnn_num_layers)])\n",
    "            \n",
    "            \n",
    "        elif args.model == 'NTM':\n",
    "            cell = NTMCell(args.rnn_size, args.memory_size, args.memory_vector_dim, 1, 1,\n",
    "                                    addressing_mode='content_and_location',\n",
    "                                    reuse=reuse,\n",
    "                                    output_dim=args.vector_dim)\n",
    "        \n",
    "        #initialize all the states\n",
    "        state = cell.zero_state(args.batch_size, tf.float32)\n",
    "        \n",
    "        self.state_list = [state]\n",
    "        \n",
    "        for t in range(seq_length):\n",
    "            output, state = cell(tf.concat([self.x[:, t, :], np.zeros([args.batch_size, 1])], axis=1), state)\n",
    "            self.state_list.append(state)\n",
    "        \n",
    "        #get the output and states\n",
    "        output, state = cell(eof, state)\n",
    "        self.state_list.append(state)\n",
    "\n",
    "        self.o = []\n",
    "        \n",
    "        for t in range(seq_length):\n",
    "            output, state = cell(zero, state)\n",
    "            self.o.append(output[:, 0:args.vector_dim])\n",
    "            self.state_list.append(state)\n",
    "        \n",
    "        self.o = tf.sigmoid(tf.transpose(self.o, perm=[1, 0, 2]))\n",
    "\n",
    "        eps = 1e-8\n",
    "        \n",
    "        #calculate loss as cross entropy loss\n",
    "        self.copy_loss = -tf.reduce_mean(self.y * tf.log(self.o + eps) + (1 - self.y) * tf.log(1 - self.o + eps))\n",
    "        \n",
    "        #optimize using RMS prop optimizer\n",
    "        with tf.variable_scope('optimizer', reuse=reuse):\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(learning_rate=args.learning_rate, momentum=0.9, decay=0.95)\n",
    "            gvs = self.optimizer.compute_gradients(self.copy_loss)\n",
    "            capped_gvs = [(tf.clip_by_value(grad, -10., 10.), var) for grad, var in gvs]\n",
    "            self.train_op = self.optimizer.apply_gradients(capped_gvs)\n",
    "                        \n",
    "        self.copy_loss_summary = tf.summary.scalar('copy_loss_%d' % seq_length, self.copy_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--mode', default=\"train\")\n",
    "parser.add_argument('--restore_training', default=False)\n",
    "parser.add_argument('--test_seq_length', type=int, default=5)\n",
    "parser.add_argument('--model', default=\"NTM\")\n",
    "parser.add_argument('--rnn_size', default=16)\n",
    "parser.add_argument('--rnn_num_layers', default=3)\n",
    "parser.add_argument('--max_seq_length', default=5)\n",
    "parser.add_argument('--memory_size', default=16)\n",
    "parser.add_argument('--memory_vector_dim', default=5)\n",
    "parser.add_argument('--batch_size', default=5)\n",
    "parser.add_argument('--vector_dim', default=8)\n",
    "parser.add_argument('--shift_range', default=1)\n",
    "parser.add_argument('--num_epoches', default=100)\n",
    "parser.add_argument('--learning_rate', default=1e-4)\n",
    "parser.add_argument('--save_dir', default= os.getcwd())\n",
    "parser.add_argument('--tensorboard_dir', default=os.getcwd())\n",
    "args = parser.parse_args(args = [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the NTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    model_list = [NTMCopyModel(args, 1)]\n",
    "    for seq_length in range(2, args.max_seq_length + 1):\n",
    "        model_list.append(NTMCopyModel(args, seq_length, reuse=True))\n",
    "\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        if args.restore_training:\n",
    "            saver = tf.train.Saver()\n",
    "            ckpt = tf.train.get_checkpoint_state(args.save_dir + '/' + args.model)\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            tf.global_variables_initializer().run()\n",
    "        \n",
    "        #initialize summary writer for visualizing in tensorboard\n",
    "        train_writer = tf.summary.FileWriter(args.tensorboard_dir, sess.graph)\n",
    "        plt.ion()\n",
    "        plt.show()\n",
    "    \n",
    "        for b in range(args.num_epoches):\n",
    "            \n",
    "            #initialize the sequence length\n",
    "            seq_length = np.random.randint(1, args.max_seq_length + 1)\n",
    "            model = model_list[seq_length - 1]\n",
    "            \n",
    "            #generate our random input sequence as an input\n",
    "            x = generate_random_strings(args.batch_size, seq_length, args.vector_dim)\n",
    "            \n",
    "            #feed our input to the model\n",
    "            feed_dict = {model.x: x}\n",
    "           \n",
    "            if b % 100 == 0:        \n",
    "                p = 0              \n",
    "                print(\"First training batch sample\", x[p, :, :])\n",
    "                \n",
    "                #compute model output\n",
    "                print(\"Model output\",sess.run(model.o, feed_dict=feed_dict)[p, :, :])\n",
    "                state_list = sess.run(model.state_list, feed_dict=feed_dict)\n",
    "                \n",
    "                \n",
    "                if args.model == 'NTM':\n",
    "                    w_plot = []\n",
    "                    M_plot = np.concatenate([state['M'][p, :, :] for state in state_list])\n",
    "                    for state in state_list:\n",
    "                        w_plot.append(np.concatenate([state['w_list'][0][p, :], state['w_list'][1][p, :]]))\n",
    "                    \n",
    "                    #plot the weight matrix to see the attention\n",
    "                    plt.imshow(w_plot, interpolation='nearest', cmap='gray')\n",
    "                    plt.draw()\n",
    "                    plt.pause(0.001)\n",
    "                \n",
    "                #compute loss\n",
    "                copy_loss = sess.run(model.copy_loss, feed_dict=feed_dict)\n",
    "                \n",
    "                #write to summary\n",
    "                merged_summary = sess.run(model.copy_loss_summary, feed_dict=feed_dict)\n",
    "                train_writer.add_summary(merged_summary, b)\n",
    "                \n",
    "                print('batches %d, loss %g' % (b, copy_loss))\n",
    "            else:                   \n",
    "                sess.run(model.train_op, feed_dict=feed_dict)\n",
    "                \n",
    "            #save the model\n",
    "            if b % 5000 == 0 and b > 0:\n",
    "                saver.save(sess, args.save_dir + '/' + args.model + '/model.tfmodel', global_step=b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 16 and 5 for 'addressing_head_0/truediv' (op: 'RealDiv') with input shapes: [5,16,1], [5,16].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[0;32m    687\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 16 and 5 for 'addressing_head_0/truediv' (op: 'RealDiv') with input shapes: [5,16,1], [5,16].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-4cd723a02bbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-442ab6916e67>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmodel_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mNTMCopyModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mmodel_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNTMCopyModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-287c87ab8e94>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, seq_length, reuse)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-4b4d04a1a68e>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, prev_state)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'addressing_head_%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                 \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddressing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_M\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_w_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mw_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-4b4d04a1a68e>\u001b[0m in \u001b[0;36maddressing\u001b[1;34m(self, k, beta, g, s, gamma, prev_M, prev_w)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[1;31m#Compute cosine similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minner_product\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnorm_product\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;31m#now, we produce the normalized weight vector based on the similairty and the key strength (beta)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    892\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 894\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    895\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mbinary_op_wrapper_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_truediv_python3\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    987\u001b[0m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 989\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_real_div\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    990\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m_real_div\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   3339\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3340\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m-> 3341\u001b[1;33m         \"RealDiv\", x=x, y=y, name=name)\n\u001b[0m\u001b[0;32m   3342\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3343\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    788\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[0;32m   2956\u001b[0m         op_def=op_def)\n\u001b[0;32m   2957\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2958\u001b[1;33m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2959\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2960\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   2207\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2209\u001b[1;33m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2210\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2211\u001b[0m     raise RuntimeError(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m   2157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2158\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2159\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2161\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[1;34m(op, require_shape_fn)\u001b[0m\n\u001b[0;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[0;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m                                   require_shape_fn)\n\u001b[0m\u001b[0;32m    628\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[1;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[0;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 691\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions must be equal, but are 16 and 5 for 'addressing_head_0/truediv' (op: 'RealDiv') with input shapes: [5,16,1], [5,16]."
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits to some parts of the code used in this section goes to [this]( https://github.com/MarkPKCollier/NeuralTuringMachine) github repo. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
